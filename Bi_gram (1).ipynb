{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5D7ImlxMo9mJ"
      },
      "outputs": [],
      "source": [
        "# import libraries needed, read the dataset\n",
        "import nltk, re, pprint, string\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "string.punctuation = string.punctuation +'“'+'”'+'-'+'’'+'‘'+'—'\n",
        "string.punctuation = string.punctuation.replace('.', '')\n",
        "file = open('/content/sample_data/corpus.txt', encoding = 'utf8').read()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocess data\n",
        "file_nl_removed = \"\"\n",
        "for line in file:\n",
        "  line_nl_removed = line.replace(\"\\n\", \" \")      #removes newlines\n",
        "  file_nl_removed += line_nl_removed\n",
        "file_p = \"\".join([char for char in file_nl_removed if char not in string.punctuation])   #removes all special characters"
      ],
      "metadata": {
        "id": "zdsvI2QkreSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIEaWl-IrntI",
        "outputId": "5f9d4da9-cd33-4903-c892-1707fcfc1406"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sents = nltk.sent_tokenize(file_p)\n",
        "print(\"The number of sentences is\", len(sents)) \n",
        "#prints the number of sentences\n",
        "words = nltk.word_tokenize(file_p)\n",
        "print(\"The number of tokens is\", len(words)) \n",
        "#prints the number of tokens\n",
        "average_tokens = round(len(words)/len(sents))\n",
        "print(\"The average number of tokens per sentence is\",\n",
        "average_tokens) \n",
        "#prints the average number of tokens per sentence\n",
        "unique_tokens = set(words)\n",
        "print(\"The number of unique tokens are\", len(unique_tokens)) \n",
        "#prints the number of unique tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWjj867Hrhd0",
        "outputId": "046b8b5e-3a9c-44c0-e3ea-cc1d7012463d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of sentences is 981\n",
            "The number of tokens is 27361\n",
            "The average number of tokens per sentence is 28\n",
            "The number of unique tokens are 3039\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoQhyhhwrz4K",
        "outputId": "a8226958-2e36-4b96-9ddf-2e07e977c17b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "unigram=[]\n",
        "bigram=[]\n",
        "trigram=[]\n",
        "fourgram=[]\n",
        "tokenized_text = []\n",
        "for sentence in sents:\n",
        "    sentence = sentence.lower()\n",
        "    sequence = word_tokenize(sentence) \n",
        "    for word in sequence:\n",
        "        if (word =='.'):\n",
        "            sequence.remove(word) \n",
        "        else:\n",
        "            unigram.append(word)\n",
        "    tokenized_text.append(sequence) \n",
        "    bigram.extend(list(ngrams(sequence, 2)))  \n",
        "#unigram, bigram, trigram, and fourgram models are created\n",
        " \n",
        "\n",
        "def removal(x):     \n",
        "#removes ngrams containing only stopwords\n",
        "    y = []\n",
        "    for pair in x:\n",
        "        count = 0\n",
        "        for word in pair:\n",
        "            if word in stop_words:\n",
        "                count = count or 0\n",
        "            else:\n",
        "                count = count or 1\n",
        "        if (count==1):\n",
        "            y.append(pair)\n",
        "    return(y)\n",
        "bigram = removal(bigram)\n",
        "freq_bi = nltk.FreqDist(bigram)\n",
        "freq_tri = nltk.FreqDist(trigram)\n",
        "freq_four = nltk.FreqDist(fourgram)\n",
        "print(\"Most common n-grams without stopword removal and without add-1 smoothing: \\n\")\n",
        "print (\"Most common bigrams: \", freq_bi.most_common(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuPKRn_mrqLH",
        "outputId": "f6e89ca2-ad0a-4c54-c565-eeac6051d213"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most common n-grams without stopword removal and without add-1 smoothing: \n",
            "\n",
            "Most common bigrams:  [(('said', 'the'), 209), (('said', 'alice'), 115), (('the', 'queen'), 65), (('the', 'king'), 60), (('a', 'little'), 59)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#stopwords = code for downloading stop words through nltk\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "#prints top 10 unigrams, bigrams after removing stopwords\n",
        "print(\"Most common n-grams with stopword removal and without add-1 smoothing: \\n\")\n",
        "unigram_sw_removed = [p for p in unigram if p not in stop_words]\n",
        "fdist = nltk.FreqDist(unigram_sw_removed)\n",
        "print(\"Most common unigrams: \", fdist.most_common(10))\n",
        "bigram_sw_removed = []\n",
        "bigram_sw_removed.extend(list(ngrams(unigram_sw_removed, 2)))\n",
        "fdist = nltk.FreqDist(bigram_sw_removed)\n",
        "print(\"\\nMost common bigrams: \", fdist.most_common(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyyxdJrvsOtH",
        "outputId": "b47c3e87-1deb-4cfc-b9b9-996e5e0cc186"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most common n-grams with stopword removal and without add-1 smoothing: \n",
            "\n",
            "Most common unigrams:  [('said', 462), ('alice', 385), ('little', 128), ('one', 101), ('like', 85), ('know', 85), ('would', 83), ('went', 83), ('could', 77), ('thought', 74)]\n",
            "\n",
            "Most common bigrams:  [(('said', 'alice'), 122), (('mock', 'turtle'), 54), (('march', 'hare'), 31), (('said', 'king'), 29), (('thought', 'alice'), 26), (('white', 'rabbit'), 22), (('said', 'hatter'), 22), (('said', 'mock'), 20), (('said', 'caterpillar'), 18), (('said', 'gryphon'), 18)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Add-1 smoothing is performed here.\n",
        "            \n",
        "ngrams_all = {1:[], 2:[]}\n",
        "for i in range(2):\n",
        "    for each in tokenized_text:\n",
        "        for j in ngrams(each, i+1):\n",
        "            ngrams_all[i+1].append(j);\n",
        "ngrams_voc = {1:set([]), 2:set([])}\n",
        "for i in range(2):\n",
        "    for gram in ngrams_all[i+1]:\n",
        "        if gram not in ngrams_voc[i+1]:\n",
        "            ngrams_voc[i+1].add(gram)\n",
        "total_ngrams = {1:-1, 2:-1}\n",
        "total_voc = {1:-1, 2:-1}\n",
        "for i in range(2):\n",
        "    total_ngrams[i+1] = len(ngrams_all[i+1])\n",
        "    total_voc[i+1] = len(ngrams_voc[i+1])                       \n",
        "    \n",
        "ngrams_prob = {1:[], 2:[]}\n",
        "for i in range(2):\n",
        "    for ngram in ngrams_voc[i+1]:\n",
        "        tlist = [ngram]\n",
        "        tlist.append(ngrams_all[i+1].count(ngram))\n",
        "        ngrams_prob[i+1].append(tlist)\n",
        "    \n",
        "for i in range(2):\n",
        "    for ngram in ngrams_prob[i+1]:\n",
        "        ngram[-1] = (ngram[-1]+1)/(total_ngrams[i+1]+total_voc[i+1])             #add-1 smoothing"
      ],
      "metadata": {
        "id": "5K-UmryIsVxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Prints top 10 unigram, bigram\n",
        "print(\"Most common n-grams without stopword removal and with add-1 smoothing: \\n\")\n",
        "for i in range(2):\n",
        "    ngrams_prob[i+1] = sorted(ngrams_prob[i+1], key = lambda x:x[1], reverse = True)\n",
        "    \n",
        "print (\"Most common unigrams: \", str(ngrams_prob[1][:10]))\n",
        "print (\"\\nMost common bigrams: \", str(ngrams_prob[2][:10]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qc4DdhYARJ-F",
        "outputId": "9e9d857e-58d4-4849-9663-5efacfa58284"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most common n-grams without stopword removal and with add-1 smoothing: \n",
            "\n",
            "Most common unigrams:  [[('the',), 0.05598462224968249], [('and',), 0.02900490852298081], [('to',), 0.02478289225277177], [('a',), 0.02155631071293722], [('she',), 0.018467030515223287], [('it',), 0.018089451824391582], [('of',), 0.017471595784848797], [('said',), 0.015892630350461675], [('i',), 0.013764459547592077], [('alice',), 0.013249579514639755]]\n",
            "\n",
            "Most common bigrams:  [[('said', 'the'), 0.0053395713087035016], [('of', 'the'), 0.0033308754354293268], [('said', 'alice'), 0.0029494774848076483], [('in', 'a'), 0.002491799944061634], [('and', 'the'), 0.002059548933357065], [('in', 'the'), 0.0020086958732741743], [('it', 'was'), 0.0019069897531083933], [('to', 'the'), 0.0017798571029011671], [('the', 'queen'), 0.0016781509827353861], [('as', 'she'), 0.0015764448625696051]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "str1 = 'after that alice said the'\n",
        "str2 = 'alice felt so desperate that she was'"
      ],
      "metadata": {
        "id": "vSXDq88QRUM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_1 = word_tokenize(str1)\n",
        "token_2 = word_tokenize(str2)\n",
        "ngram_1 = {1:[], 2:[]}   #to store the n-grams formed  \n",
        "ngram_2 = {1:[], 2:[]}\n",
        "for i in range(2):\n",
        "    ngram_1[i+1] = list(ngrams(token_1, i+1))[-1]\n",
        "    ngram_2[i+1] = list(ngrams(token_2, i+1))[-1]\n",
        "print(\"String 1: \", ngram_1,\"\\nString 2: \",ngram_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQLRiEmGRYvu",
        "outputId": "e1f7eea5-8f17-4e20-81a8-98727bf55b47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "String 1:  {1: ('the',), 2: ('said', 'the')} \n",
            "String 2:  {1: ('was',), 2: ('she', 'was')}\n"
          ]
        }
      ]
    }
  ]
}